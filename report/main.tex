\documentclass[12pt]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Spring 2018\\  ECE563 Large Course Project}
\author{Charitha Saumya Gusthinna Waduge (cgusthin@purdue.edu) \\ Ehab Mohammad Ghabashneh (eghabash@purdue.edu)}

\begin{document}
\maketitle
\section{Overall Strategy}
\subsection{OpenMP implementation}
\label{omp-impl}
\subsection{MPI implementation}
In the MPI implementation we split the files in {\em RawText } directory among the available MPI processes. The distribution of files is done by a master process ( process $0$).
Master process read all the filenames in the input directory and stores them in a list. Then later sends $m$  number of files to each worker process in a round robin fashion as long as the file
list is not empty. If the file list is empty master sends a special {\em ``all done!''} message to all worker processes. Each worker process keeps receiving files in to its local file list until
it receives the {\em``all done!''} message. After all the files are received, a worker run its own instance of OpenMP map reduce which is described in section~\ref{omp-impl}. The results of the
local map reduce is written in to $P$ number of files where $P$ is the number of worker processes. 

\noindent \textbf{Sender Thread} : In the next step each worker initiate a sender thread and receiver thread. Sender threads job is
to send the local result to the worker who is responsible for the global reduction. For example if process $P1$ is responsible for counting the word {\em ``always''} all the other processes send their
local counts of that word to $P1$ and $P1$ does the final reduction. In our implementation the word count hash table is split among the processes uniformly i.e. if the hash table size is $M$ each worker gets 
to reduce $N/P$ of words where $P$ is the number of workers. If the sender is done with a local result that needs to be sent to a specific worker it will send a special {\em ``end of file''} message. This
lets the receiving end know that it received the complete result from this sender. 

\noindent \textbf{Receiver Thread} : The receiver thread does the final reduction on the set of words assigned it. Receiver keeps receiving local results from all the other processes until it 
receives the {\em ``end of file''} messages from all 
other workers. At the end receiver writes it final reduction result to a file. Therefore at the end there will be $P$ files containing the final result each corresponding to specific portion of the word 
count hash table. 

\noindent \textbf{Communication Strategy} : We used a fairly simple communication strategy for the whole MPI implementation. For the file name distribution, since only the master process is sending and all others are 
receiving its straight forward to use {\em MPI\_Recv}'s at the receiving end. But things get little complicated in the reduction step because every process need to send different data to every other process. This makes 
it difficult to know in what order the messages will be received by a given process. We solved this problem by using {\em MPI\_Recv}'s with {\em MPI\_ANY\_SOURCE} as the source and {\em MPI\_Status} tells us who actually sent 
the message.
\section{Evaluation}
\subsection{Speedup and Efficiency}
\subsection{Karp-Flatt Analysis}
\subsection{Thread Performance}
\subsection{Performance Bottlenecks}
\subsection{Suggested Improvements}
%\begin{table*}[ht]
%\centering
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{|l|l|l|l|l|l|}
%\hline
%no of vertices	 & no of edges   	& graph density		 & Floyd-Warshall 		& Johnson's 			& Hybrid  \\
				 %&  			 	& ($|E|/|E_{max}|$)	 & (ms)			    	& (ms)					& (ms)  \\ \hline \hline
%50 				 & 269  			& 0.1  				 & 5.548 				& 4.395 				& 4.330 \\ \hline 
%50 				 & 1242 	       	& 0.5 				 & 5.825  				& 8.504 				& 5.776	\\ \hline 
%50 				 & 2201 			& 0.9 				 & 5.754				& 1.224 x $10^1$		& 5.736 \\ \hline \hline

%100 			 & 985  			& 0.1  				 & 4.444 $\times 10^1$ 	& 2.13 $\times 10^1$ 	& 2.192 $\times 10^1$ \\ \hline 
%100 			 & 4833 	       	& 0.5 				 & 4.426 x $10^1$  		& 5.258 $\times 10^1$ 	& 4.425 $\times 10^1$	\\ \hline 
%100 		     & 8944 			& 0.9 				 & 4.446 x $10^1$		& 8.501 $\times 10^1$	& 4.434 $\times 10^1$ \\ \hline \hline

%200 			 & 4014  			& 0.1  				 & 3.440 $\times 10^2$ 	& 1.229 $\times 10^2$ 	& 1.229 $\times 10^2$ \\ \hline 
%200 			 & 19873 	       	& 0.5 				 & 3.464 $\times 10^2$  & 3.827 $\times 10^2$ 	& 3.510 $\times 10^2$	\\ \hline 
%200 		     & 35915			& 0.9 				 & 3.469 $\times 10^2$	& 6.368 $\times 10^2$	& 3.466 $\times 10^2$ \\ \hline \hline

%500 			 & 25258  			& 0.1  				 & 5.28 $\times 10^3$ 	& 1.422 $\times 10^3$ 	& 1.423 $\times 10^3$ \\ \hline 
%500 			 & 124751 	       	& 0.5 				 & 5.294 $\times 10^3$  & 5.236 $\times 10^3$ 	& 5.3 $\times 10^3$	\\ \hline 
%500 		     & 224482			& 0.9 				 & 5.295 $\times 10^3$	& 9.228 $\times 10^3$	& 5.286 $\times 10^3$ \\ \hline \hline

%700 			 & 49040  			& 0.1  				 & 1.462 $\times 10^4$ 	& 3.6 $\times 10^3$ 	& 3.604 $\times 10^3$ \\ \hline 
%700 			 & 244364 	       	& 0.5 				 & 1.465 $\times 10^4$  & 1.463 $\times 10^4$ 	& 1.444 $\times 10^4$	\\ \hline 
%700 		     & 440254			& 0.9 				 & 1.442 $\times 10^4$	& 2.544 $\times 10^4$	& 1.442 $\times 10^4$ \\ \hline \hline

%1000 			 & 99475  			& 0.1  				 & 4.186 $\times 10^4$ 	& 1.008 $\times 10^4$ 	& 1.007 $\times 10^4$ \\ \hline 
%1000 			 & 499377 	       	& 0.5 				 & 4.187 $\times 10^4$  & 4.289 $\times 10^4$ 	& 4.179 $\times 10^4$	\\ \hline 
%1000 		     & 899099			& 0.9 				 & 4.189 $\times 10^4$	& 7.467 $\times 10^4$	& 4.183 $\times 10^4$ \\ \hline 



%\end{tabular}
%}
%\caption{Running time comparison of the algorithms for graphs with different graph densities}
%\label{eval}
%\end{table*}
%\bibliographystyle{acm}
%\bibliography{sample}

\end{document}
